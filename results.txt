coarse: (batch size 1000, 500 test steps)
    [30], adam, sigmoid, squared error, 
        3000 - 0.75, 0.61, 0.98 //pretty bad
    [30], adam, sigmoid, squared error, normalize inputs, 
        1500 - 0.91, 0.87, 0.92 //5 bad, not great with green buoys, 
    [30, 10], adam, sigmoid, squared error, normalize inputs, 
        3000 - 0.94, 0.90, 0.95 //9 bad, not great with green buoys, 
    [30, 30], adam, sigmoid, squared error, normalize inputs, 
        3000 - 0.94, 0.91, 0.94
    [30, 10], adam, relu and sigmoid, squared error, normalize inputs, 
        3000 - 0.94, 0.89, 0.96 //11 bad, not great with green buoys, very binary, several restarts, 
    [30, 10], adam, prelu and sigmoid, squared error, normalize inputs, 
        3000 - 0.93, 0.89, 0.95 //9 bad, not great with green buoys, several restarts, 
    [30, 10], adam, sigmoid, softmax cross entropy with logits, normalize inputs, 
        3000 - 0.93, 0.87, 0.96 //reaches 0.90 quickly, 9 bad, very binary, not great with green buoys, 
    [30, 10], adam, relu and sigmoid, softmax cross entropy with logits, normalize inputs, several restarts, 
        3000 - 0.94, 0.89, 0.95 //10 bad, very binary, not great with green buoys, 
    [30, 10], adam, sigmoid, softmax cross entropy with logits, normalize inputs, sharpen input, 
        3000 - 0.93, 0.89, 0.94
    [30, 10], adam, sigmoid, softmax cross entropy with logits, normalize inputs, use edge finder, 
        3000 - 0.93, 0.88, 0.95 //not great
    [30, 10], adam, sigmoid, squared error, normalize inputs, dropout 0.75
        3000 - 0.93, 0.86, 0.96
    use rps ~0.2:
        [30, 10], adam, sigmoid, squared error, normalize inputs, 
            3000 - 0.94, 0.82, 0.90 //mostly ok
            //current
    use rps ~0.2, then rps ~0.13:
        [30, 10], adam, sigmoid, squared error, normalize inputs, 
            3000, 3000 - 0.97, 0.84, 0.95 //not much better

detailed: (batch size 50, 500 test steps)
    conv (5x5->32, max, 5x5->64, max), dense (64->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.91, 0.92, 0.88 //missed buoys are far away, some bad false positives
    conv (5x5->32, max, 5x5->64, max), dense (32->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.91, 0.93, 0.86
    conv (5x5->32, max, 5x5->32, max), dense (32->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.90, 0.89, 0.88
    conv (5x5->16, max, 5x5->32, max), dense (32->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.92, 0.91, 0.90
    conv (5x5->16, max, 5x5->16, max), dense (32->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.87, 0.82, 0.91
    conv (5x5->16, max, 5x5->32, max), dense (16->2), dropout 0.5, cross entropy cost, adam, normalise input
        3000 - 0.88, 0.84, 0.91
    conv (5x5->16, max, 5x5->32, max), dense (32->2), dropout 0.75, cross entropy cost, adam, normalise input
        3000 - 0.90, 0.88, 0.89
        6000 - 0.89, 0.94, 0.79
        7000 - 0.92, 0.93, 0.88
    conv (5x5->16, max, 5x5->32, max), dense (32->2), dropout 0.9, cross entropy cost, adam, normalise input
        3000 - 0.91, 0.90, 0.90
    conv (5x5->16, max, 5x5->32, max), dense (64->2), dropout 0.9, cross entropy cost, adam, normalise input
        3000 - 0.92, 0.91, 0.91
    modified training/validation/testing set:
        conv (5x5->16, max, 5x5->32, max), dense (64->2), dropout 0.9, cross entropy cost, adam, normalise input
            32x32 static cell size:
                partial buoy containment:
                    3000 - 0.92, 0.97, 0.89
                    6000 - 0.91, 0.97, 0.87
                    10000 - 0.90, 0.97, 0.85
                full buoy containment:
                    3000 - 0.94, 0.91, 0.88
                    6000 - 0.94, 0.96, 0.81
            variable cell size (450-700, 1/2 and 1/2, ((y-400)*(175/(300)) + 15) / 2):
                partial buoy containment:
                    3000 - 0.95, 0.95, 0.98
                    6000 - 0.94, 0.98, 0.94
            reduced ratio of positive samples (to about 0.35):
                variable cell size (450-700, 1/2 and 1/2, ((y-400)*(175/(300)) + 15) / 2):
                    partial buoy containment:
                        3000 - 0.94, 0.93, 0.93
            reduced ratio of positive samples (to about 0.3):
                32x32 static cell size:
                    partial buoy containment:
                        3000 - 0.92, 0.98, 0.71
                variable cell size (450-700, 1/2 and 1/2, ((y-400)*(175/(300)) + 15) / 2):
                    partial buoy containment:
                        3000 - 0.95, 0.85, 0.92
            use ~0.3 rps (rotate only), then train with ~0.08 rps (nothing):
                variable cell size (450-700, 1/2 and 1/2, ((y-400)*(175/(300)) + 15) / 2):
                    32x32 static cell size:
                        3000, 3000 - 0.97, 0.66, 0.54
                    partial buoy containment:
                        3000, 3000 - 0.94, 0.86, 0.92 //ok
            sliding window, 32x32, 0.4 to 0.7, 1/2 and 1/2, [2.0, 4.0, 6.0]
                rps ~0.25 (rskip 0.9), then ~0.1 (rskip 0.7), then ~0.04 rps (rskip 0.7, no rotate)
                3000             - 0.96, 0.95, 0.82
                3000, 3000       - 0.98, 0.86, 0.73
                3000, 3000, 3000 - 0.99, 0.49, 0.44 //ok
            sliding window, 32x32, [1.0,1.5,2.5], [0.42,0.44,0.5] & [0.55,0.65,0.68], 5/12 & 5/12, 
                rps ~0.25, then ~0.1, then ~0.04 rps
                3000, 3000, 3000 - 0.99, 0.54, 0.45
                //current
    2 object types:
        [16, 32], 1, 2, [64], relu, dropout 0.9, cross entropy cost, adam, normalise input
            sliding window, 32x32, [1.0,1.5,2.5], [0.42,0.44,0.5] & [0.55,0.65,0.68], 5/12 & 5/12, 
                rps [0.3, 0.3, 0.4] then [0.03, 0.03, 0.94]
                    2000, 3000 - 0.98, 0.93, 0.64 //ok
                rps [0.25, 0.25, 0.5] then [0.1, 0.1, 0.8] then [0.04, 0.04, 0.92]
                    3000             - 0.75, 0.74, 0.52
                    3000, 3000       - 0.93, 0.95, 0.67
                    3000, 3000, 3000 - 0.97, 0.96, 0.63 //ok
